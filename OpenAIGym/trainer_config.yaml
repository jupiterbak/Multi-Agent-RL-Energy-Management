default:
    trainer: DQN            # trainer type
    batch_size: 1024        # batch size
    memory_size: 1024       # allocated memory size
    beta: 5.0e-3
    epsilon: 0.2            # exploration rate
    epsilon_min: 0.01       # minimal exploration rate
    epsilon_decay: 0.995
    gamma: 0.95             # discount factor
    tau: .125               # soft target model updates
    num_layers: 2           # number of hidden layers
    hidden_units: 128       # number of hidden unit in the hidden layer
    use_recurrent: false    # use R-NN
    lambd: 0.95
    learning_rate: 0.001    # learning rate
    max_steps: 500000       # maximum number of state for an episode
    normalize: false        # add nomalization layer to the associated NN
    num_epoch: 5000         # total number of epochs
    time_horizon: 64        # time horizon
    sequence_length: 64     # TODO:
    summary_freq: 1000      # Frequency at which at summary will be printed out
    buffer_size: 10240

RealFakeBrain:
    trainer: Random         # trainer type
    batch_size: 512         # batch size
    memory_size: 10240      # allocated memory size
    beta: 5.0e-3
    epsilon: 0.2            # exploration rate
    epsilon_min: 0.01       # minimal exploration rate
    epsilon_decay: 0.995
    gamma: 0.99             # discount factor
    tau: .001               # soft target model updates
    num_layers: 2           # number of hidden layers
    hidden_units: 64        # number of hidden unit in the hidden layer
    use_recurrent: false    # use R-NN
    lambd: 0.95
    learning_rate: 0.001    # learning rate
    max_steps: 5000000      # maximum number of state for an episode
    normalize: false        # add nomalization layer to the associated NN
    num_epoch: 10000        # total number of epochs
    time_horizon: 64        # time horizon
    sequence_length: 64     # TODO:
    summary_freq: 1000      # Frequency at which at summary will be printed out

MCD:
    trainer: Random         # trainer type
    batch_size: 512         # batch size
    memory_size: 10240      # allocated memory size
    beta: 5.0e-3
    epsilon: 0.2            # exploration rate
    epsilon_min: 0.01       # minimal exploration rate
    epsilon_decay: 0.995
    gamma: 0.99             # discount factor
    tau: .001               # soft target model updates
    num_layers: 2           # number of hidden layers
    hidden_units: 32        # number of hidden unit in the hidden layer
    use_recurrent: false    # use R-NN
    lambd: 0.95
    learning_rate: 0.001    # learning rate
    max_steps: 5000000      # maximum number of state for an episode
    normalize: false        # add normalization layer to the associated NN
    num_epoch: 10000        # total number of epochs
    time_horizon: 64        # time horizon
    sequence_length: 64     # TODO:
    summary_freq: 1000      # Frequency at which at summary will be printed out